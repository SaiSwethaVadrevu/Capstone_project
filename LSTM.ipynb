{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqa0hsJ7rzsE",
        "outputId": "e2f4a420-51b2-45c9-c83f-ad453ab85227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# load data\n",
        "data_path = '/content/drive/MyDrive/Team7/ForwardKeys_data.csv'\n",
        "df = pd.read_csv(data_path)"
      ],
      "metadata": {
        "id": "pMVyRivcr_tj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Team7/ForwardKeys_data.csv')\n",
        "\n",
        "# Convert date and datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')  # Adjust format if necessary\n",
        "df['DayOfWeek'] = df['Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
        "\n",
        "# Create one-hot encodings for day of the week\n",
        "dayofweek_onehot = pd.get_dummies(df['DayOfWeek'], prefix='Day')\n",
        "df = df.join(dayofweek_onehot)\n",
        "\n",
        "# all location\n",
        "locations = ['Visitors in Blue Lagoon', 'Visitors in Machu Picchu', 'Visitors in Taj Mahal', 'Visitors in Doge\\'s Palace', 'Visitors in Louvre Museum']\n",
        "\n",
        "# hold sequence data for all location and time slot combinations\n",
        "sequence_data_all = {}\n",
        "\n",
        "# normalize\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "for location in locations:\n",
        "    for time_slot in df['Time'].unique():\n",
        "\n",
        "        # filter data for current location and time slot\n",
        "        df_filtered = df[df['Time'] == time_slot][['Date', location] + dayofweek_onehot.columns.tolist()].copy()\n",
        "        df_filtered[location] = scaler.fit_transform(df_filtered[[location]])\n",
        "\n",
        "        # make data\n",
        "        sequence_data = df_filtered[[location] + dayofweek_onehot.columns.tolist()].astype(np.float32)\n",
        "\n",
        "        # create sequences\n",
        "        def create_sequences(data, seq_length):\n",
        "            xs, ys = [], []\n",
        "            for i in range(len(data) - seq_length):\n",
        "                x = data.iloc[i:(i + seq_length)].values\n",
        "                y = data.iloc[i + seq_length, 0]\n",
        "                xs.append(x)\n",
        "                ys.append(y)\n",
        "            return np.array(xs), np.array(ys)\n",
        "\n",
        "        seq_length = 50\n",
        "        X, y = create_sequences(sequence_data, seq_length)\n",
        "\n",
        "        # convert PyTorch tensors\n",
        "        X_tensor = torch.from_numpy(X)\n",
        "        y_tensor = torch.from_numpy(y).view(-1, 1)\n",
        "        sequence_data_all[(location, time_slot)] = (X_tensor, y_tensor)\n"
      ],
      "metadata": {
        "id": "pOAeWZPIsIpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Taj Mahal\n",
        "# LSTM model class\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# train and eva\n",
        "def train_evaluate_model(X_train, X_test, y_train, y_test, time_slot):\n",
        "    lstm_model = LSTMModel(input_dim=X_train.shape[-1], hidden_dim=64, num_layers=3, output_dim=1)\n",
        "    lstm_criterion = nn.MSELoss()\n",
        "    lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "    lstm_num_epochs = 170\n",
        "    for epoch in range(lstm_num_epochs):\n",
        "        lstm_model.train()\n",
        "        lstm_optimizer.zero_grad()\n",
        "        lstm_output = lstm_model(X_train)\n",
        "        lstm_loss = lstm_criterion(lstm_output, y_train)\n",
        "        lstm_loss.backward()\n",
        "        lstm_optimizer.step()\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Taj Mahal - Epoch {epoch}, Loss: {lstm_loss.item()} at {time_slot}')\n",
        "\n",
        "    lstm_model.eval()\n",
        "    with torch.no_grad():\n",
        "        lstm_predictions = lstm_model(X_test)\n",
        "        lstm_r_squared = r2_score(y_test.numpy(), lstm_predictions.numpy())\n",
        "        lstm_rmse = np.sqrt(mean_squared_error(y_test.numpy(), lstm_predictions.numpy()))\n",
        "\n",
        "        print(f'Time Slot: {time_slot}, LSTM R-squared: {lstm_r_squared}, RMSE: {lstm_rmse}')\n",
        "\n",
        "location = 'Visitors in Taj Mahal'\n",
        "time_slots = ['8:00', '10:00', '12:00', '14:00', '16:00', '18:00']\n",
        "for time_slot in time_slots:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(*sequence_data_all[(location, time_slot)], test_size=0.2, random_state=42)\n",
        "    train_evaluate_model(X_train, X_test, y_train, y_test, time_slot)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l2Zer4_wI2k",
        "outputId": "a58e202e-ddf8-471b-8d6d-c27e2939e895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taj Mahal - Epoch 0, Loss: 0.34845834970474243 at 8:00\n",
            "Taj Mahal - Epoch 10, Loss: 0.1133299320936203 at 8:00\n",
            "Taj Mahal - Epoch 20, Loss: 0.1090138852596283 at 8:00\n",
            "Taj Mahal - Epoch 30, Loss: 0.10659698396921158 at 8:00\n",
            "Taj Mahal - Epoch 40, Loss: 0.10611901432275772 at 8:00\n",
            "Taj Mahal - Epoch 50, Loss: 0.10499054193496704 at 8:00\n",
            "Taj Mahal - Epoch 60, Loss: 0.10329072922468185 at 8:00\n",
            "Taj Mahal - Epoch 70, Loss: 0.10054298490285873 at 8:00\n",
            "Taj Mahal - Epoch 80, Loss: 0.094386026263237 at 8:00\n",
            "Taj Mahal - Epoch 90, Loss: 0.0755375400185585 at 8:00\n",
            "Taj Mahal - Epoch 100, Loss: 0.03833676874637604 at 8:00\n",
            "Taj Mahal - Epoch 110, Loss: 0.029527096077799797 at 8:00\n",
            "Taj Mahal - Epoch 120, Loss: 0.023243196308612823 at 8:00\n",
            "Taj Mahal - Epoch 130, Loss: 0.0177286509424448 at 8:00\n",
            "Taj Mahal - Epoch 140, Loss: 0.013891847804188728 at 8:00\n",
            "Taj Mahal - Epoch 150, Loss: 0.012651271186769009 at 8:00\n",
            "Taj Mahal - Epoch 160, Loss: 0.012071150355041027 at 8:00\n",
            "Time Slot: 8:00, LSTM R-squared: 0.8779144423136276, RMSE: 0.1111702248454094\n",
            "Taj Mahal - Epoch 0, Loss: 0.22350575029850006 at 10:00\n",
            "Taj Mahal - Epoch 10, Loss: 0.11264582723379135 at 10:00\n",
            "Taj Mahal - Epoch 20, Loss: 0.10689358413219452 at 10:00\n",
            "Taj Mahal - Epoch 30, Loss: 0.1040773093700409 at 10:00\n",
            "Taj Mahal - Epoch 40, Loss: 0.10344085842370987 at 10:00\n",
            "Taj Mahal - Epoch 50, Loss: 0.10201667994260788 at 10:00\n",
            "Taj Mahal - Epoch 60, Loss: 0.09886545687913895 at 10:00\n",
            "Taj Mahal - Epoch 70, Loss: 0.09048352390527725 at 10:00\n",
            "Taj Mahal - Epoch 80, Loss: 0.05739980190992355 at 10:00\n",
            "Taj Mahal - Epoch 90, Loss: 0.03287886083126068 at 10:00\n",
            "Taj Mahal - Epoch 100, Loss: 0.023376839235424995 at 10:00\n",
            "Taj Mahal - Epoch 110, Loss: 0.0164997186511755 at 10:00\n",
            "Taj Mahal - Epoch 120, Loss: 0.013155709020793438 at 10:00\n",
            "Taj Mahal - Epoch 130, Loss: 0.01219207514077425 at 10:00\n",
            "Taj Mahal - Epoch 140, Loss: 0.01195062231272459 at 10:00\n",
            "Taj Mahal - Epoch 150, Loss: 0.011855519376695156 at 10:00\n",
            "Taj Mahal - Epoch 160, Loss: 0.01174933835864067 at 10:00\n",
            "Time Slot: 10:00, LSTM R-squared: 0.8841072914909108, RMSE: 0.11067456007003784\n",
            "Taj Mahal - Epoch 0, Loss: 0.17520453035831451 at 12:00\n",
            "Taj Mahal - Epoch 10, Loss: 0.11628551036119461 at 12:00\n",
            "Taj Mahal - Epoch 20, Loss: 0.10919660329818726 at 12:00\n",
            "Taj Mahal - Epoch 30, Loss: 0.10657493770122528 at 12:00\n",
            "Taj Mahal - Epoch 40, Loss: 0.10489185899496078 at 12:00\n",
            "Taj Mahal - Epoch 50, Loss: 0.10041467100381851 at 12:00\n",
            "Taj Mahal - Epoch 60, Loss: 0.08328329771757126 at 12:00\n",
            "Taj Mahal - Epoch 70, Loss: 0.03282001614570618 at 12:00\n",
            "Taj Mahal - Epoch 80, Loss: 0.023814544081687927 at 12:00\n",
            "Taj Mahal - Epoch 90, Loss: 0.018001630902290344 at 12:00\n",
            "Taj Mahal - Epoch 100, Loss: 0.013916915282607079 at 12:00\n",
            "Taj Mahal - Epoch 110, Loss: 0.012111512944102287 at 12:00\n",
            "Taj Mahal - Epoch 120, Loss: 0.011632287874817848 at 12:00\n",
            "Taj Mahal - Epoch 130, Loss: 0.01155172474682331 at 12:00\n",
            "Taj Mahal - Epoch 140, Loss: 0.011507858522236347 at 12:00\n",
            "Taj Mahal - Epoch 150, Loss: 0.011457344517111778 at 12:00\n",
            "Taj Mahal - Epoch 160, Loss: 0.011418401263654232 at 12:00\n",
            "Time Slot: 12:00, LSTM R-squared: 0.8907412355399482, RMSE: 0.11067038029432297\n",
            "Taj Mahal - Epoch 0, Loss: 0.23516695201396942 at 14:00\n",
            "Taj Mahal - Epoch 10, Loss: 0.10656485706567764 at 14:00\n",
            "Taj Mahal - Epoch 20, Loss: 0.10587318241596222 at 14:00\n",
            "Taj Mahal - Epoch 30, Loss: 0.10440394282341003 at 14:00\n",
            "Taj Mahal - Epoch 40, Loss: 0.10328685492277145 at 14:00\n",
            "Taj Mahal - Epoch 50, Loss: 0.10082948952913284 at 14:00\n",
            "Taj Mahal - Epoch 60, Loss: 0.09548847377300262 at 14:00\n",
            "Taj Mahal - Epoch 70, Loss: 0.07911185920238495 at 14:00\n",
            "Taj Mahal - Epoch 80, Loss: 0.03493731841444969 at 14:00\n",
            "Taj Mahal - Epoch 90, Loss: 0.026841558516025543 at 14:00\n",
            "Taj Mahal - Epoch 100, Loss: 0.02057041972875595 at 14:00\n",
            "Taj Mahal - Epoch 110, Loss: 0.01562795415520668 at 14:00\n",
            "Taj Mahal - Epoch 120, Loss: 0.01316035259515047 at 14:00\n",
            "Taj Mahal - Epoch 130, Loss: 0.012161479331552982 at 14:00\n",
            "Taj Mahal - Epoch 140, Loss: 0.011776037514209747 at 14:00\n",
            "Taj Mahal - Epoch 150, Loss: 0.011576341465115547 at 14:00\n",
            "Taj Mahal - Epoch 160, Loss: 0.011423207819461823 at 14:00\n",
            "Time Slot: 14:00, LSTM R-squared: 0.8908955648446445, RMSE: 0.1041460707783699\n",
            "Taj Mahal - Epoch 0, Loss: 0.22040249407291412 at 16:00\n",
            "Taj Mahal - Epoch 10, Loss: 0.10630479454994202 at 16:00\n",
            "Taj Mahal - Epoch 20, Loss: 0.10607815533876419 at 16:00\n",
            "Taj Mahal - Epoch 30, Loss: 0.10489650070667267 at 16:00\n",
            "Taj Mahal - Epoch 40, Loss: 0.10288071632385254 at 16:00\n",
            "Taj Mahal - Epoch 50, Loss: 0.09826074540615082 at 16:00\n",
            "Taj Mahal - Epoch 60, Loss: 0.08461522310972214 at 16:00\n",
            "Taj Mahal - Epoch 70, Loss: 0.036732498556375504 at 16:00\n",
            "Taj Mahal - Epoch 80, Loss: 0.02784021571278572 at 16:00\n",
            "Taj Mahal - Epoch 90, Loss: 0.021946262568235397 at 16:00\n",
            "Taj Mahal - Epoch 100, Loss: 0.016436129808425903 at 16:00\n",
            "Taj Mahal - Epoch 110, Loss: 0.013328591361641884 at 16:00\n",
            "Taj Mahal - Epoch 120, Loss: 0.011837226338684559 at 16:00\n",
            "Taj Mahal - Epoch 130, Loss: 0.01105749886482954 at 16:00\n",
            "Taj Mahal - Epoch 140, Loss: 0.010828842408955097 at 16:00\n",
            "Taj Mahal - Epoch 150, Loss: 0.010716907680034637 at 16:00\n",
            "Taj Mahal - Epoch 160, Loss: 0.01061179954558611 at 16:00\n",
            "Time Slot: 16:00, LSTM R-squared: 0.8599754632806815, RMSE: 0.11121062189340591\n",
            "Taj Mahal - Epoch 0, Loss: 0.4035986363887787 at 18:00\n",
            "Taj Mahal - Epoch 10, Loss: 0.14220963418483734 at 18:00\n",
            "Taj Mahal - Epoch 20, Loss: 0.10283360630273819 at 18:00\n",
            "Taj Mahal - Epoch 30, Loss: 0.10207891464233398 at 18:00\n",
            "Taj Mahal - Epoch 40, Loss: 0.10134200751781464 at 18:00\n",
            "Taj Mahal - Epoch 50, Loss: 0.10027332603931427 at 18:00\n",
            "Taj Mahal - Epoch 60, Loss: 0.09911028295755386 at 18:00\n",
            "Taj Mahal - Epoch 70, Loss: 0.09783206135034561 at 18:00\n",
            "Taj Mahal - Epoch 80, Loss: 0.09579457342624664 at 18:00\n",
            "Taj Mahal - Epoch 90, Loss: 0.09161267429590225 at 18:00\n",
            "Taj Mahal - Epoch 100, Loss: 0.08096808195114136 at 18:00\n",
            "Taj Mahal - Epoch 110, Loss: 0.04938219115138054 at 18:00\n",
            "Taj Mahal - Epoch 120, Loss: 0.03721584752202034 at 18:00\n",
            "Taj Mahal - Epoch 130, Loss: 0.02711702510714531 at 18:00\n",
            "Taj Mahal - Epoch 140, Loss: 0.02114180475473404 at 18:00\n",
            "Taj Mahal - Epoch 150, Loss: 0.016324877738952637 at 18:00\n",
            "Taj Mahal - Epoch 160, Loss: 0.012962672859430313 at 18:00\n",
            "Time Slot: 18:00, LSTM R-squared: 0.8832746483068736, RMSE: 0.11254274845123291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Louvre Museum\n",
        "#LsTM\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
        "df['DayOfWeek'] = df['Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
        "\n",
        "# Create one hot encodings for day of the week\n",
        "dayofweek_onehot = pd.get_dummies(df['DayOfWeek'], prefix='Day')\n",
        "df = pd.concat([df, dayofweek_onehot], axis=1)\n",
        "\n",
        "# normalize\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "location = 'Visitors in Louvre Museum'\n",
        "time_slots = ['8:00', '10:00', '12:00', '14:00', '16:00', '18:00']\n",
        "sequence_data_all = {}\n",
        "\n",
        "for time_slot in time_slots:\n",
        "    df_filtered = df[df['Time'] == time_slot][['Date', location] + dayofweek_onehot.columns.tolist()].copy()\n",
        "    df_filtered[location] = scaler.fit_transform(df_filtered[[location]])\n",
        "    sequence_data = df_filtered[[location] + dayofweek_onehot.columns.tolist()].astype(np.float32)\n",
        "\n",
        "    # Create sequences\n",
        "    def create_sequences(data, seq_length=50):\n",
        "        xs, ys = [], []\n",
        "        for i in range(len(data) - seq_length):\n",
        "            x = data.iloc[i:(i + seq_length)].values\n",
        "            y = data.iloc[i + seq_length, 0]\n",
        "            xs.append(x)\n",
        "            ys.append(y)\n",
        "        return np.array(xs), np.array(ys)\n",
        "\n",
        "    X, y = create_sequences(sequence_data)\n",
        "    X_tensor = torch.from_numpy(X).float()\n",
        "    y_tensor = torch.from_numpy(y).float().view(-1, 1)\n",
        "    sequence_data_all[time_slot] = (X_tensor, y_tensor)\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "def train_evaluate_model(X_train, X_test, y_train, y_test):\n",
        "    lstm_model = LSTMModel(input_dim=X_train.shape[-1], hidden_dim=64, num_layers=3, output_dim=1)\n",
        "    lstm_criterion = nn.MSELoss()\n",
        "    lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "    lstm_num_epochs = 170\n",
        "    for epoch in range(lstm_num_epochs):\n",
        "        lstm_model.train()\n",
        "        lstm_optimizer.zero_grad()\n",
        "        lstm_output = lstm_model(X_train)\n",
        "        lstm_loss = lstm_criterion(lstm_output, y_train)\n",
        "        lstm_loss.backward()\n",
        "        lstm_optimizer.step()\n",
        "\n",
        "    lstm_model.eval()\n",
        "    with torch.no_grad():\n",
        "        lstm_predictions = lstm_model(X_test)\n",
        "\n",
        "for time_slot in time_slots:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(sequence_data_all[time_slot][0], sequence_data_all[time_slot][1], test_size=0.2, random_state=42)\n",
        "    train_evaluate_model(X_train, X_test, y_train, y_test)\n"
      ],
      "metadata": {
        "id": "QMAljKUoBYTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Blue lagoon\n",
        "# LSTM\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# train and eva\n",
        "def train_evaluate_model(X_train, X_test, y_train, y_test, time_slot, location):\n",
        "    lstm_model = LSTMModel(input_dim=X_train.shape[-1], hidden_dim=64, num_layers=3, output_dim=1)\n",
        "    lstm_criterion = nn.MSELoss()\n",
        "    lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "    lstm_num_epochs = 170\n",
        "    for epoch in range(lstm_num_epochs):\n",
        "        lstm_model.train()\n",
        "        lstm_optimizer.zero_grad()\n",
        "        lstm_output = lstm_model(X_train)\n",
        "        lstm_loss = lstm_criterion(lstm_output, y_train)\n",
        "        lstm_loss.backward()\n",
        "        lstm_optimizer.step()\n",
        "\n",
        "\n",
        "    lstm_model.eval()\n",
        "    with torch.no_grad():\n",
        "        lstm_predictions = lstm_model(X_test)\n",
        "\n",
        "location = 'Visitors in Blue Lagoon'\n",
        "time_slots = ['8:00', '10:00', '12:00', '14:00', '16:00', '18:00']\n",
        "for time_slot in time_slots:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(*sequence_data_all[(location, time_slot)], test_size=0.2, random_state=42)\n",
        "    train_evaluate_model(X_train, X_test, y_train, y_test, time_slot, location)\n"
      ],
      "metadata": {
        "id": "10qpNFNj27YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Machu Picchu\n",
        "# LSTM\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# train and eva\n",
        "def train_evaluate_model(X_train, X_test, y_train, y_test, time_slot, location):\n",
        "    lstm_model = LSTMModel(input_dim=X_train.shape[-1], hidden_dim=64, num_layers=3, output_dim=1)\n",
        "    lstm_criterion = nn.MSELoss()\n",
        "    lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "    lstm_num_epochs = 170\n",
        "    for epoch in range(lstm_num_epochs):\n",
        "        lstm_model.train()\n",
        "        lstm_optimizer.zero_grad()\n",
        "        lstm_output = lstm_model(X_train)\n",
        "        lstm_loss = lstm_criterion(lstm_output, y_train)\n",
        "        lstm_loss.backward()\n",
        "        lstm_optimizer.step()\n",
        "\n",
        "\n",
        "    lstm_model.eval()\n",
        "    with torch.no_grad():\n",
        "        lstm_predictions = lstm_model(X_test)\n",
        "        lstm_r_squared = r2_score(y_test.numpy(), lstm_predictions.numpy())\n",
        "\n",
        "location = 'Visitors in Machu Picchu'\n",
        "time_slots = ['8:00', '10:00', '12:00', '14:00', '16:00', '18:00']\n",
        "\n",
        "for time_slot in time_slots:\n",
        "    if time_slot in sequence_data_all:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(*sequence_data_all[time_slot], test_size=0.2, random_state=42)\n",
        "        train_evaluate_model(X_train, X_test, y_train, y_test, time_slot, location)\n",
        "    else:\n",
        "        print(f\"Data not found for {location} at {time_slot}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "yUWa748c3UYE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequence_data_all.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_ZDLoaNttN5",
        "outputId": "f8045597-4d6e-4147-dfef-5acfa9ee7be8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['8:00', '10:00', '12:00', '14:00', '16:00', '18:00'])\n"
          ]
        }
      ]
    }
  ]
}