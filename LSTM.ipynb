{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uqa0hsJ7rzsE",
    "outputId": "35b0d303-0f4f-4136-ca03-9f41532fd601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pMVyRivcr_tj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# load data\n",
    "data_path = '/content/drive/MyDrive/Team7/ForwardKeys_data.csv'\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pOAeWZPIsIpy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('/content/drive/MyDrive/Team7/ForwardKeys_data.csv')\n",
    "\n",
    "# Convert date and datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')  # Adjust format if necessary\n",
    "df['DayOfWeek'] = df['Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "# Create one-hot encodings for day of the week\n",
    "dayofweek_onehot = pd.get_dummies(df['DayOfWeek'], prefix='Day')\n",
    "df = df.join(dayofweek_onehot)\n",
    "\n",
    "# all location\n",
    "locations = ['Visitors in Blue Lagoon', 'Visitors in Machu Picchu', 'Visitors in Taj Mahal', 'Visitors in Doge\\'s Palace', 'Visitors in Louvre Museum']\n",
    "\n",
    "# hold sequence data for all location and time slot combinations\n",
    "sequence_data_all = {}\n",
    "\n",
    "# normalize\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for location in locations:\n",
    "    for time_slot in df['Time'].unique():\n",
    "\n",
    "        # filter data for current location and time slot\n",
    "        df_filtered = df[df['Time'] == time_slot][['Date', location] + dayofweek_onehot.columns.tolist()].copy()\n",
    "        df_filtered[location] = scaler.fit_transform(df_filtered[[location]])\n",
    "\n",
    "        # make data\n",
    "        sequence_data = df_filtered[[location] + dayofweek_onehot.columns.tolist()].astype(np.float32)\n",
    "\n",
    "        # create sequences\n",
    "        def create_sequences(data, seq_length):\n",
    "            xs, ys = [], []\n",
    "            for i in range(len(data) - seq_length):\n",
    "                x = data.iloc[i:(i + seq_length)].values\n",
    "                y = data.iloc[i + seq_length, 0]\n",
    "                xs.append(x)\n",
    "                ys.append(y)\n",
    "            return np.array(xs), np.array(ys)\n",
    "\n",
    "        seq_length = 50\n",
    "        X, y = create_sequences(sequence_data, seq_length)\n",
    "\n",
    "        # convert PyTorch tensors\n",
    "        X_tensor = torch.from_numpy(X)\n",
    "        y_tensor = torch.from_numpy(y).view(-1, 1)\n",
    "        sequence_data_all[(location, time_slot)] = (X_tensor, y_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5l2Zer4_wI2k",
    "outputId": "a58e202e-ddf8-471b-8d6d-c27e2939e895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taj Mahal - Epoch 0, Loss: 0.34845834970474243 at 8:00\n",
      "Taj Mahal - Epoch 10, Loss: 0.1133299320936203 at 8:00\n",
      "Taj Mahal - Epoch 20, Loss: 0.1090138852596283 at 8:00\n",
      "Taj Mahal - Epoch 30, Loss: 0.10659698396921158 at 8:00\n",
      "Taj Mahal - Epoch 40, Loss: 0.10611901432275772 at 8:00\n",
      "Taj Mahal - Epoch 50, Loss: 0.10499054193496704 at 8:00\n",
      "Taj Mahal - Epoch 60, Loss: 0.10329072922468185 at 8:00\n",
      "Taj Mahal - Epoch 70, Loss: 0.10054298490285873 at 8:00\n",
      "Taj Mahal - Epoch 80, Loss: 0.094386026263237 at 8:00\n",
      "Taj Mahal - Epoch 90, Loss: 0.0755375400185585 at 8:00\n",
      "Taj Mahal - Epoch 100, Loss: 0.03833676874637604 at 8:00\n",
      "Taj Mahal - Epoch 110, Loss: 0.029527096077799797 at 8:00\n",
      "Taj Mahal - Epoch 120, Loss: 0.023243196308612823 at 8:00\n",
      "Taj Mahal - Epoch 130, Loss: 0.0177286509424448 at 8:00\n",
      "Taj Mahal - Epoch 140, Loss: 0.013891847804188728 at 8:00\n",
      "Taj Mahal - Epoch 150, Loss: 0.012651271186769009 at 8:00\n",
      "Taj Mahal - Epoch 160, Loss: 0.012071150355041027 at 8:00\n",
      "Time Slot: 8:00, LSTM R-squared: 0.8779144423136276, RMSE: 0.1111702248454094\n",
      "Taj Mahal - Epoch 0, Loss: 0.22350575029850006 at 10:00\n",
      "Taj Mahal - Epoch 10, Loss: 0.11264582723379135 at 10:00\n",
      "Taj Mahal - Epoch 20, Loss: 0.10689358413219452 at 10:00\n",
      "Taj Mahal - Epoch 30, Loss: 0.1040773093700409 at 10:00\n",
      "Taj Mahal - Epoch 40, Loss: 0.10344085842370987 at 10:00\n",
      "Taj Mahal - Epoch 50, Loss: 0.10201667994260788 at 10:00\n",
      "Taj Mahal - Epoch 60, Loss: 0.09886545687913895 at 10:00\n",
      "Taj Mahal - Epoch 70, Loss: 0.09048352390527725 at 10:00\n",
      "Taj Mahal - Epoch 80, Loss: 0.05739980190992355 at 10:00\n",
      "Taj Mahal - Epoch 90, Loss: 0.03287886083126068 at 10:00\n",
      "Taj Mahal - Epoch 100, Loss: 0.023376839235424995 at 10:00\n",
      "Taj Mahal - Epoch 110, Loss: 0.0164997186511755 at 10:00\n",
      "Taj Mahal - Epoch 120, Loss: 0.013155709020793438 at 10:00\n",
      "Taj Mahal - Epoch 130, Loss: 0.01219207514077425 at 10:00\n",
      "Taj Mahal - Epoch 140, Loss: 0.01195062231272459 at 10:00\n",
      "Taj Mahal - Epoch 150, Loss: 0.011855519376695156 at 10:00\n",
      "Taj Mahal - Epoch 160, Loss: 0.01174933835864067 at 10:00\n",
      "Time Slot: 10:00, LSTM R-squared: 0.8841072914909108, RMSE: 0.11067456007003784\n",
      "Taj Mahal - Epoch 0, Loss: 0.17520453035831451 at 12:00\n",
      "Taj Mahal - Epoch 10, Loss: 0.11628551036119461 at 12:00\n",
      "Taj Mahal - Epoch 20, Loss: 0.10919660329818726 at 12:00\n",
      "Taj Mahal - Epoch 30, Loss: 0.10657493770122528 at 12:00\n",
      "Taj Mahal - Epoch 40, Loss: 0.10489185899496078 at 12:00\n",
      "Taj Mahal - Epoch 50, Loss: 0.10041467100381851 at 12:00\n",
      "Taj Mahal - Epoch 60, Loss: 0.08328329771757126 at 12:00\n",
      "Taj Mahal - Epoch 70, Loss: 0.03282001614570618 at 12:00\n",
      "Taj Mahal - Epoch 80, Loss: 0.023814544081687927 at 12:00\n",
      "Taj Mahal - Epoch 90, Loss: 0.018001630902290344 at 12:00\n",
      "Taj Mahal - Epoch 100, Loss: 0.013916915282607079 at 12:00\n",
      "Taj Mahal - Epoch 110, Loss: 0.012111512944102287 at 12:00\n",
      "Taj Mahal - Epoch 120, Loss: 0.011632287874817848 at 12:00\n",
      "Taj Mahal - Epoch 130, Loss: 0.01155172474682331 at 12:00\n",
      "Taj Mahal - Epoch 140, Loss: 0.011507858522236347 at 12:00\n",
      "Taj Mahal - Epoch 150, Loss: 0.011457344517111778 at 12:00\n",
      "Taj Mahal - Epoch 160, Loss: 0.011418401263654232 at 12:00\n",
      "Time Slot: 12:00, LSTM R-squared: 0.8907412355399482, RMSE: 0.11067038029432297\n",
      "Taj Mahal - Epoch 0, Loss: 0.23516695201396942 at 14:00\n",
      "Taj Mahal - Epoch 10, Loss: 0.10656485706567764 at 14:00\n",
      "Taj Mahal - Epoch 20, Loss: 0.10587318241596222 at 14:00\n",
      "Taj Mahal - Epoch 30, Loss: 0.10440394282341003 at 14:00\n",
      "Taj Mahal - Epoch 40, Loss: 0.10328685492277145 at 14:00\n",
      "Taj Mahal - Epoch 50, Loss: 0.10082948952913284 at 14:00\n",
      "Taj Mahal - Epoch 60, Loss: 0.09548847377300262 at 14:00\n",
      "Taj Mahal - Epoch 70, Loss: 0.07911185920238495 at 14:00\n",
      "Taj Mahal - Epoch 80, Loss: 0.03493731841444969 at 14:00\n",
      "Taj Mahal - Epoch 90, Loss: 0.026841558516025543 at 14:00\n",
      "Taj Mahal - Epoch 100, Loss: 0.02057041972875595 at 14:00\n",
      "Taj Mahal - Epoch 110, Loss: 0.01562795415520668 at 14:00\n",
      "Taj Mahal - Epoch 120, Loss: 0.01316035259515047 at 14:00\n",
      "Taj Mahal - Epoch 130, Loss: 0.012161479331552982 at 14:00\n",
      "Taj Mahal - Epoch 140, Loss: 0.011776037514209747 at 14:00\n",
      "Taj Mahal - Epoch 150, Loss: 0.011576341465115547 at 14:00\n",
      "Taj Mahal - Epoch 160, Loss: 0.011423207819461823 at 14:00\n",
      "Time Slot: 14:00, LSTM R-squared: 0.8908955648446445, RMSE: 0.1041460707783699\n",
      "Taj Mahal - Epoch 0, Loss: 0.22040249407291412 at 16:00\n",
      "Taj Mahal - Epoch 10, Loss: 0.10630479454994202 at 16:00\n",
      "Taj Mahal - Epoch 20, Loss: 0.10607815533876419 at 16:00\n",
      "Taj Mahal - Epoch 30, Loss: 0.10489650070667267 at 16:00\n",
      "Taj Mahal - Epoch 40, Loss: 0.10288071632385254 at 16:00\n",
      "Taj Mahal - Epoch 50, Loss: 0.09826074540615082 at 16:00\n",
      "Taj Mahal - Epoch 60, Loss: 0.08461522310972214 at 16:00\n",
      "Taj Mahal - Epoch 70, Loss: 0.036732498556375504 at 16:00\n",
      "Taj Mahal - Epoch 80, Loss: 0.02784021571278572 at 16:00\n",
      "Taj Mahal - Epoch 90, Loss: 0.021946262568235397 at 16:00\n",
      "Taj Mahal - Epoch 100, Loss: 0.016436129808425903 at 16:00\n",
      "Taj Mahal - Epoch 110, Loss: 0.013328591361641884 at 16:00\n",
      "Taj Mahal - Epoch 120, Loss: 0.011837226338684559 at 16:00\n",
      "Taj Mahal - Epoch 130, Loss: 0.01105749886482954 at 16:00\n",
      "Taj Mahal - Epoch 140, Loss: 0.010828842408955097 at 16:00\n",
      "Taj Mahal - Epoch 150, Loss: 0.010716907680034637 at 16:00\n",
      "Taj Mahal - Epoch 160, Loss: 0.01061179954558611 at 16:00\n",
      "Time Slot: 16:00, LSTM R-squared: 0.8599754632806815, RMSE: 0.11121062189340591\n",
      "Taj Mahal - Epoch 0, Loss: 0.4035986363887787 at 18:00\n",
      "Taj Mahal - Epoch 10, Loss: 0.14220963418483734 at 18:00\n",
      "Taj Mahal - Epoch 20, Loss: 0.10283360630273819 at 18:00\n",
      "Taj Mahal - Epoch 30, Loss: 0.10207891464233398 at 18:00\n",
      "Taj Mahal - Epoch 40, Loss: 0.10134200751781464 at 18:00\n",
      "Taj Mahal - Epoch 50, Loss: 0.10027332603931427 at 18:00\n",
      "Taj Mahal - Epoch 60, Loss: 0.09911028295755386 at 18:00\n",
      "Taj Mahal - Epoch 70, Loss: 0.09783206135034561 at 18:00\n",
      "Taj Mahal - Epoch 80, Loss: 0.09579457342624664 at 18:00\n",
      "Taj Mahal - Epoch 90, Loss: 0.09161267429590225 at 18:00\n",
      "Taj Mahal - Epoch 100, Loss: 0.08096808195114136 at 18:00\n",
      "Taj Mahal - Epoch 110, Loss: 0.04938219115138054 at 18:00\n",
      "Taj Mahal - Epoch 120, Loss: 0.03721584752202034 at 18:00\n",
      "Taj Mahal - Epoch 130, Loss: 0.02711702510714531 at 18:00\n",
      "Taj Mahal - Epoch 140, Loss: 0.02114180475473404 at 18:00\n",
      "Taj Mahal - Epoch 150, Loss: 0.016324877738952637 at 18:00\n",
      "Taj Mahal - Epoch 160, Loss: 0.012962672859430313 at 18:00\n",
      "Time Slot: 18:00, LSTM R-squared: 0.8832746483068736, RMSE: 0.11254274845123291\n"
     ]
    }
   ],
   "source": [
    "#Taj Mahal\n",
    "# LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# train and eva\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test, time_slot):\n",
    "    lstm_model = LSTMModel(input_dim=X_train.shape[-1], hidden_dim=64, num_layers=3, output_dim=1)\n",
    "    lstm_criterion = nn.MSELoss()\n",
    "    lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "    lstm_num_epochs = 170\n",
    "    for epoch in range(lstm_num_epochs):\n",
    "        lstm_model.train()\n",
    "        lstm_optimizer.zero_grad()\n",
    "        lstm_output = lstm_model(X_train)\n",
    "        lstm_loss = lstm_criterion(lstm_output, y_train)\n",
    "        lstm_loss.backward()\n",
    "        lstm_optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Taj Mahal - Epoch {epoch}, Loss: {lstm_loss.item()} at {time_slot}')\n",
    "\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        lstm_predictions = lstm_model(X_test)\n",
    "        lstm_r_squared = r2_score(y_test.numpy(), lstm_predictions.numpy())\n",
    "        lstm_rmse = np.sqrt(mean_squared_error(y_test.numpy(), lstm_predictions.numpy()))\n",
    "\n",
    "        print(f'Time Slot: {time_slot}, LSTM R-squared: {lstm_r_squared}, RMSE: {lstm_rmse}')\n",
    "\n",
    "location = 'Visitors in Taj Mahal'\n",
    "time_slots = ['8:00', '10:00', '12:00', '14:00', '16:00', '18:00']\n",
    "for time_slot in time_slots:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(*sequence_data_all[(location, time_slot)], test_size=0.2, random_state=42)\n",
    "    train_evaluate_model(X_train, X_test, y_train, y_test, time_slot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QMAljKUoBYTg"
   },
   "outputs": [],
   "source": [
    "#Louvre Museum\n",
    "#LsTM\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "df['DayOfWeek'] = df['Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "# Create one hot encodings for day of the week\n",
    "dayofweek_onehot = pd.get_dummies(df['DayOfWeek'], prefix='Day')\n",
    "df = pd.concat([df, dayofweek_onehot], axis=1)\n",
    "\n",
    "# normalize\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "location = 'Visitors in Louvre Museum'\n",
    "time_slots = ['8:00', '10:00', '12:00', '14:00', '16:00', '18:00']\n",
    "sequence_data_all = {}\n",
    "\n",
    "for time_slot in time_slots:\n",
    "    df_filtered = df[df['Time'] == time_slot][['Date', location] + dayofweek_onehot.columns.tolist()].copy()\n",
    "    df_filtered[location] = scaler.fit_transform(df_filtered[[location]])\n",
    "    sequence_data = df_filtered[[location] + dayofweek_onehot.columns.tolist()].astype(np.float32)\n",
    "\n",
    "    # Create sequences\n",
    "    def create_sequences(data, seq_length=50):\n",
    "        xs, ys = [], []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            x = data.iloc[i:(i + seq_length)].values\n",
    "            y = data.iloc[i + seq_length, 0]\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "        return np.array(xs), np.array(ys)\n",
    "\n",
    "    X, y = create_sequences(sequence_data)\n",
    "    X_tensor = torch.from_numpy(X).float()\n",
    "    y_tensor = torch.from_numpy(y).float().view(-1, 1)\n",
    "    sequence_data_all[time_slot] = (X_tensor, y_tensor)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test):\n",
    "    lstm_model = LSTMModel(input_dim=X_train.shape[-1], hidden_dim=64, num_layers=3, output_dim=1)\n",
    "    lstm_criterion = nn.MSELoss()\n",
    "    lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "    lstm_num_epochs = 170\n",
    "    for epoch in range(lstm_num_epochs):\n",
    "        lstm_model.train()\n",
    "        lstm_optimizer.zero_grad()\n",
    "        lstm_output = lstm_model(X_train)\n",
    "        lstm_loss = lstm_criterion(lstm_output, y_train)\n",
    "        lstm_loss.backward()\n",
    "        lstm_optimizer.step()\n",
    "\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        lstm_predictions = lstm_model(X_test)\n",
    "\n",
    "for time_slot in time_slots:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sequence_data_all[time_slot][0], sequence_data_all[time_slot][1], test_size=0.2, random_state=42)\n",
    "    train_evaluate_model(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10qpNFNj27YV"
   },
   "outputs": [],
   "source": [
    "# Blue lagoon\n",
    "# LSTM\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# train and eva\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test, time_slot, location):\n",
    "    lstm_model = LSTMModel(input_dim=X_train.shape[-1], hidden_dim=64, num_layers=3, output_dim=1)\n",
    "    lstm_criterion = nn.MSELoss()\n",
    "    lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "    lstm_num_epochs = 170\n",
    "    for epoch in range(lstm_num_epochs):\n",
    "        lstm_model.train()\n",
    "        lstm_optimizer.zero_grad()\n",
    "        lstm_output = lstm_model(X_train)\n",
    "        lstm_loss = lstm_criterion(lstm_output, y_train)\n",
    "        lstm_loss.backward()\n",
    "        lstm_optimizer.step()\n",
    "\n",
    "\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        lstm_predictions = lstm_model(X_test)\n",
    "\n",
    "location = 'Visitors in Blue Lagoon'\n",
    "time_slots = ['8:00', '10:00', '12:00', '14:00', '16:00', '18:00']\n",
    "for time_slot in time_slots:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(*sequence_data_all[(location, time_slot)], test_size=0.2, random_state=42)\n",
    "    train_evaluate_model(X_train, X_test, y_train, y_test, time_slot, location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "yUWa748c3UYE",
    "outputId": "d9200b67-7af8-4e68-82d6-6e51073dd24e"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('Visitors in Machu Picchu', '8:00')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-200fa089d78b>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mtime_slots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'8:00'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'10:00'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'12:00'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'14:00'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'16:00'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'18:00'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtime_slot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtime_slots\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msequence_data_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_slot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mtrain_evaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_slot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('Visitors in Machu Picchu', '8:00')"
     ]
    }
   ],
   "source": [
    "# Machu Picchu\n",
    "# LSTM\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# train and eva\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test, time_slot, location):\n",
    "    lstm_model = LSTMModel(input_dim=X_train.shape[-1], hidden_dim=64, num_layers=3, output_dim=1)\n",
    "    lstm_criterion = nn.MSELoss()\n",
    "    lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "    lstm_num_epochs = 170\n",
    "    for epoch in range(lstm_num_epochs):\n",
    "        lstm_model.train()\n",
    "        lstm_optimizer.zero_grad()\n",
    "        lstm_output = lstm_model(X_train)\n",
    "        lstm_loss = lstm_criterion(lstm_output, y_train)\n",
    "        lstm_loss.backward()\n",
    "        lstm_optimizer.step()\n",
    "\n",
    "\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        lstm_predictions = lstm_model(X_test)\n",
    "        lstm_r_squared = r2_score(y_test.numpy(), lstm_predictions.numpy())\n",
    "\n",
    "location = 'Visitors in Machu Picchu'\n",
    "time_slots = ['8:00', '10:00', '12:00', '14:00', '16:00', '18:00']\n",
    "for time_slot in time_slots:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(*sequence_data_all[(location, time_slot)], test_size=0.2, random_state=42)\n",
    "    train_evaluate_model(X_train, X_test, y_train, y_test, time_slot, location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "Gk--sk4n3vhK",
    "outputId": "39be4b2b-1ff1-4696-c6ff-cf7b0cb2b1c8"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(\"Visitors in Doge's Palace\", '8:00')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-19843095d499>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mtime_slots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'8:00'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'10:00'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'12:00'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'14:00'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'16:00'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'18:00'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtime_slot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtime_slots\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msequence_data_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_slot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mtrain_evaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_slot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (\"Visitors in Doge's Palace\", '8:00')"
     ]
    }
   ],
   "source": [
    "# Doge's Palace\n",
    "# LSTM\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# train and eva\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test, time_slot, location):\n",
    "    lstm_model = LSTMModel(input_dim=X_train.shape[-1], hidden_dim=64, num_layers=3, output_dim=1)\n",
    "    lstm_criterion = nn.MSELoss()\n",
    "    lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "    lstm_num_epochs = 170\n",
    "    for epoch in range(lstm_num_epochs):\n",
    "        lstm_model.train()\n",
    "        lstm_optimizer.zero_grad()\n",
    "        lstm_output = lstm_model(X_train)\n",
    "        lstm_loss = lstm_criterion(lstm_output, y_train)\n",
    "        lstm_loss.backward()\n",
    "        lstm_optimizer.step()\n",
    "\n",
    "\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        lstm_predictions = lstm_model(X_test)  # You might use these predictions later\n",
    "\n",
    "location = 'Visitors in Doge\\'s Palace'\n",
    "time_slots = ['8:00', '10:00', '12:00', '14:00', '16:00', '18:00']\n",
    "for time_slot in time_slots:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(*sequence_data_all[(location, time_slot)], test_size=0.2, random_state=42)\n",
    "    train_evaluate_model(X_train, X_test, y_train, y_test, time_slot, location)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
